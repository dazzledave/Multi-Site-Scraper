# Multi-Site Product Scraper

A modern web application that allows users to search for products across multiple e-commerce platforms simultaneously. Built with Flask and featuring a beautiful, responsive UI.

![image](https://github.com/user-attachments/assets/8fb5dbcd-7059-477d-8e6e-ca217d78ef65)


##  Features

- **Multi-Platform Search**: Search across multiple e-commerce sites at once
- **Beautiful UI**: Modern, responsive design with dark/light theme toggle
- **Real-time Results**: Get product information, prices, and links instantly
- **Scraper Selection**: Choose which platforms to search from
- **Error Handling**: Robust error handling and retry mechanisms

## 🛠️ Currently Supported Platforms

- ✅ **Jumia** - African e-commerce platform
- 🔄 **Amazon** - Coming Soon
- 🔄 **eBay** - Coming Soon
- 🔄 **AliExpress** - Coming Soon
- 🔄 **Walmart** - Coming Soon

## 🚀 Quick Start

### Prerequisites

- Python 3.7 or higher
- ScraperAPI account (for web scraping)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/multi-site-product-scraper.git
   cd multi-site-product-scraper
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up your ScraperAPI key**
   - Get your API key from [ScraperAPI](https://www.scraperapi.com/)
   - Set it as an environment variable:
     ```bash
     export SCRAPERAPI_KEY=your_api_key_here
     ```
   - Or update the default key in the scraper files

4. **Run the application**
   ```bash
   python app.py
   ```

5. **Open your browser**
   - Go to `http://127.0.0.1:5000`
   - Start searching for products!

## 📁 Project Structure

```
multi-site-product-scraper/
├── app.py                 # Main Flask application
├── jumia_scraper.py      # Jumia scraper implementation
├── requirements.txt      # Python dependencies
├── templates/           # HTML templates
│   ├── index.html      # Main search page
│   └── results.html    # Results display page
├── README.md           # This file
└── .gitignore         # Git ignore rules
```

### Adding New Scrapers

To add a new scraper:

1. Create a new scraper file (e.g., `amazon_scraper.py`)
2. Implement the scraping function
3. Add it to the `AVAILABLE_SCRAPERS` configuration in `app.py`
4. Update the UI templates if needed

### Environment Variables

- `SCRAPERAPI_KEY`: Your ScraperAPI key for web scraping

## Error Handling

The application includes comprehensive error handling:
- Network timeout retries
- Multiple ScraperAPI configurations
- Graceful fallbacks for failed scrapers
- User-friendly error messages

##  Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

##  License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ⚠️ Disclaimer

This tool is for educational purposes. Please respect the terms of service of the websites you're scraping and ensure you have permission to access their data.

## Support

If you encounter any issues:
1. Check the debug files generated by the scrapers
2. Verify your ScraperAPI key is valid and has credits
3. Check the console output for error messages
4. Open an issue on GitHub with details

## 🔮 Future Enhancements

- [ ] Add more e-commerce platforms
- [ ] Implement product comparison features
- [ ] Add price tracking and alerts
- [ ] Create mobile app version
- [ ] Add export to CSV/Excel functionality
- [ ] Implement user accounts and search history 
